{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# import input_data\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "R = tf.Variable(\"float\", [None, n_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _simple_lrp(R, X, W, b):\n",
    "    Z = tf.expand_dims(W, 0) * tf.expand_dims(X, -1)\n",
    "    Zs = tf.add(tf.expand_dims(tf.reduce_sum(Z, 1), 1),\n",
    "                tf.expand_dims(tf.expand_dims(b, 0), 0))\n",
    "    return tf.reduce_sum(\n",
    "            (Z / Zs) * tf.expand_dims(R, 1),\n",
    "            2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# relevance propagation\n",
    "def relevance_propagation(y_pred, \n",
    "                          reversed_layers_inputs,\n",
    "                          reversed_layers_weights,\n",
    "                          reversed_layers_biases):\n",
    "    assert len(reversed_layers_inputs) == len(reversed_layers_weights) == len(reversed_layers_biases)\n",
    "    R = y_pred\n",
    "    for i in range(len(reversed_layers_inputs)):\n",
    "        R = _simple_lrp(R, \n",
    "                        reversed_layers_inputs[i],\n",
    "                        reversed_layers_weights[i],\n",
    "                        reversed_layers_biases[i])\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron_lrp(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1_activations = tf.nn.tanh(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1_activations, weights['h2']), biases['b2'])\n",
    "    layer_2_activations = tf.nn.tanh(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2_activations, weights['out']) + biases['out']\n",
    "    \n",
    "    R = relevance_propagation(out_layer, \n",
    "                              [layer_2_activations, layer_1_activations, x],\n",
    "                              [weights['out'], weights['h2'], weights['h1']],\n",
    "                              [biases['out'], biases['b2'], biases['b1']])\n",
    "    \n",
    "    return out_layer, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred, R = multilayer_perceptron_lrp(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 6.688744279\n",
      "Epoch: 0002 cost= 2.491919826\n",
      "Epoch: 0003 cost= 1.679120137\n",
      "Epoch: 0004 cost= 1.224639658\n",
      "Epoch: 0005 cost= 0.907718809\n",
      "Epoch: 0006 cost= 0.696063237\n",
      "Epoch: 0007 cost= 0.540805862\n",
      "Epoch: 0008 cost= 0.422097471\n",
      "Epoch: 0009 cost= 0.329363246\n",
      "Epoch: 0010 cost= 0.255429530\n",
      "Epoch: 0011 cost= 0.202367658\n",
      "Epoch: 0012 cost= 0.174606755\n",
      "Epoch: 0013 cost= 0.147124677\n",
      "Epoch: 0014 cost= 0.122132774\n",
      "Epoch: 0015 cost= 0.106500016\n",
      "Optimization Finished!\n",
      "Accuracy: 0.8809000253677368\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy: {}\".format(accuracy.eval({x: mnist.test.images, y: mnist.test.labels})))\n",
    "    \n",
    "    saver.save(sess, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.imshow(mnist.test.images[0].reshape(28, 28), cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "[[ -5.19088411 -13.56048679  22.07215118  12.98138237 -21.61441803\n",
      "   -2.89621425 -16.432724    47.64886093 -17.41369057  -1.14940417]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess, model_path)\n",
    "    predictions = sess.run(pred, feed_dict={x: mnist.test.images[:1]})\n",
    "    print (predictions.shape)\n",
    "    print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   6.03327484e+01   1.83703022e+01\n",
      "   -2.35433731e+01   3.72000580e+01   2.84039726e+01  -1.07948570e+01\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.33933258e+02   1.13372803e+00\n",
      "   -1.58983879e+01   1.52848114e+02  -5.51635361e+01   1.06049103e+02\n",
      "   -6.33160400e+00  -1.37014542e+02  -1.58834213e+02   9.61524200e+01\n",
      "   -1.29898865e+02   5.29682922e+01  -1.48809158e+02   1.41645996e+02\n",
      "   -5.28902969e+01  -2.40379143e+01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.53668633e+01   8.63620377e+00\n",
      "    3.41547775e+00   2.65140381e+01  -8.12997589e+01   1.72088013e+02\n",
      "   -5.38365822e+01  -3.79710236e+01  -2.19849434e+01   5.71122208e+01\n",
      "    2.03182159e+02  -7.20943680e+01  -4.14290466e+01   6.60584793e+01\n",
      "    8.89813843e+01   1.40661993e+01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   1.51721401e+01\n",
      "    2.41638145e+01  -6.69748116e+00   2.19584408e+01  -3.41144791e+01\n",
      "   -2.51552143e+01  -4.89485779e+01  -4.35489321e+00  -5.32239151e+01\n",
      "   -1.47996216e+01  -7.59122696e+01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -1.45663729e+01   1.31291122e+02\n",
      "   -3.67050858e+01   9.82956409e-01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00  -5.45815372e+00  -8.95161438e+01   5.90752563e+01\n",
      "   -1.37655067e+01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   7.72622871e+00  -1.00589508e+02  -8.04216461e+01\n",
      "   -3.24239960e+01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    3.26085548e+01   8.39278488e+01  -2.91782990e+02  -9.29837990e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   -2.84323502e+01   4.71700363e+01   1.72268333e+01   5.06699085e-02\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00  -2.69405651e+00\n",
      "   -2.45452919e+01   9.03954697e+00   2.08058167e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   2.53769512e+01\n",
      "    2.77001266e+01  -8.53463898e+01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   3.54114189e+01   2.49618664e+01\n",
      "   -1.78942886e+02  -1.85470581e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.58526850e+00   4.88827400e+01   1.43989410e+02\n",
      "   -1.18588905e+01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    5.62332869e-02   3.16146469e+00   9.41590118e+01  -4.69693184e+01\n",
      "   -1.42995224e+01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    2.16946831e+01   1.49933365e+02   6.28193359e+01   5.43687744e+01\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   1.18052187e+01\n",
      "   -2.30883427e+01  -1.97350830e+02  -7.34447479e-01   6.23520136e-01\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   1.49909019e+01\n",
      "   -2.38116821e+02   1.18690079e+02  -1.49121132e+01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   7.98568726e-01   4.00035553e+01\n",
      "   -7.67589722e+01  -4.96731186e+01  -2.31972466e+01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -1.92527847e+01  -1.59014511e+01\n",
      "    8.03620682e+01   9.70195923e+01  -4.17769527e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00  -3.97109337e+01  -1.20619530e+02\n",
      "    7.72930222e+01  -1.59567285e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess, model_path)\n",
    "    relevance = sess.run(R, feed_dict={x: mnist.test.images[:1]})\n",
    "    print (relevance.shape)\n",
    "    print (relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import render\n",
    "import data_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving image to ./heatmap.png\n",
      "writing data in npy-format to /home/scitator/Documents/Git/lrp_toolbox/tensorflow/heatmap.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/python3/lib/python3.5/site-packages/numpy/core/fromnumeric.py:225: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return reshape(newshape, order=order)\n",
      "/opt/anaconda/envs/python3/lib/python3.5/site-packages/skimage/util/dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAADFCAYAAACfOaMVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD/tJREFUeJzt3X+wFeV9x/HP7kHAGAGJYZSCTcRKxRJihyRQZ0jQkdFx\n6jQSgx1nmDRW09EEGyakWK9FASlTIjYRGghME9PJtNZINB0tkzBFJ5JghhHJNE7ShgHB6gwIljKt\nqNzd/rH73HufPc/ee8/Pe773vF//nLPPPrvnufc+PHz2Oc/ZE6VpKgCADfFINwAAMHwM2gBgCIM2\nABjCoA0AhjBoA4AhDNoAYAiDNgAYwqANAIYwaAOAIWPa8BqPteE1gCG5T//GsZ9VetJ7R6I5wGB+\ntUbr/jq0ox2D9tI2vAYAjCbPSwoO2kyPAIAhDNoAYAiDNgAYwqANAIYwaAOAIQzaAGAIgzYAGMKg\nDQCGMGgDgCEM2gBgCIM2ABjCoA0AhjBoA4AhDNoAYAiDNgAYwqANAIYwaAOAIQzaAGAIgzYAGMKg\nDQCGMGgDgCEM2gBgCIM2ABjCoA0AhjBoA4AhDNoAYAiDNgAYwqANAIYwaAOAIQzaAGAIgzYAGMKg\nDQCGMGgDgCEM2gBgCIM2ABjCoA0AhjBoA4AhDNoAYAiDNgAYwqANAIaMGekGtEOappKkJ598Mli+\nffv24HFTp04Nlo8fPz5YfttttwXLL7roomD5ZZddFixHt8r642IV+2n2+Pvxf9Z8xijKHnckiyRJ\np3V+sN7P9bFg+SSdqvk10VokbQAwpCuSNmBKHq2vin8T3L05Wept/0QLqup8Nn1cknRz5ceSpE9H\nP/L2uwT+WLLYlRQbUUOD0U4kbQAwhKQNdAg3d11M2C8lvyNJ+pnmS5KiPCa7LBwFUnKc1/lBcp23\nx81pu9daGrn5c38evU9+6p7k3uH9EGg5kjYAGNJVSXvFihXB8sOHDzfl/Fu2bAmWT5gwIVg+a9as\nprzuSJg+fbq37dJf2e947ty5LW/TaJM8nj/5frZqZMcti4P13q/T3naUprppgz+HneaRec5X9+aV\nsoff6/2qJOkXv55XqJ9ZMfNBSdLaaFWtze8ced+ck+yRJM3SK8FqM3SwbU1qBEkbAAzpqqRd5NZp\no3H8LpvPrfDo/836c9dpsLSsrKR25L9W1XniLNctT1Z7p9m4sSd8QHHBS5QdkPRU/Nbk56nsTILt\n/OntVxXqZw28+i/3B+snMypVpVfdniXrX1SuliQdiLPHWUnxcxm2+i5JGwAM6eqkDWB4iitWqqL5\nYLE/UH2oHVFp9C+rHyjLrxI+0usn7n+M/1SS9MdVidsGkjYAGNJVSXvbtm3B8gMHDgTLy1Z3vPJK\n+N3n/fuL822Z5557Lli+d+/eYPkll1zibbv54qNHjwbr1+Kcc84Jll944YXB8jfeeCNYXtb24qoS\nh9UjQ3NpcWPvHdl2ZZtXvvaWt4PHXazXC+dJ9diKW7wyt3rkN6cu9eu6yeVNxcZkD7sevTZYft3y\np4NtmaJjherZ+V/S7wbrJ9sK8/T5D3vR7YeCr7tg3c7g+X8l/9+MJP381Hz/3Cezx/GTs2M+rhe9\n/Sf0gWAbOw1JGwAM6aqkXZwncwm2bP5suPXrLe9kFto4WsVx1l8eSbLE7f4Uy6O/rf+keR8c/9Y3\nveLSv3L/5HVwR/U/mTRYe0ADCtVL6qdlk+Nlr1t+/qqqeUR1VzLL429l28mdZSftSF01aAOW9P1n\nnz88lNwTrPeuxlaVfUiHJfWPgZ+vPCFJOjMpPAXgLrmTu+tqqil9SymN5hKmRwDAkKgNl8Ed8//Z\nSF3yv/XWW8F2vPzyy8H6xTftXP19+/Y13JZx48YFy2fOnBl8zSuuuCJYv/gzOZs2+e9oubR41113\n1dTOVnA/Uxz7WaUn7bSbITW/nw7V9T+QnpAkLa+E36zvWIWPqA/0ifTFqjJJevydz0qSTp37W5Kk\niWeyN3K/NO7RVrSwXs+v0bpPhXaQtAHAkK6a0y5dsN8itb7RWW95LRp9zaGuVtr9Ox6dmv87LP+z\n5Fcfkf/mp3N+4WZUzisKL4edqV8Hy3+om4Ll79P/BcsP60Petmv+J/Vc9iTvhhsqDxZqOGnVD+36\n7qn3TZMkTXj7dVlE0gYAQ7oqaQMYXP+VkluSV3aTqpIleW2qnzYy7191Bdwxb7sNC0kbAAwhabeQ\n+5980qRJwf0LFy6s6XzXXHNNw20qs2PHDm/bzf+VrRKZPXu2t+1+1iVLlrSgdWid7O/2piYH975Z\n8tHu8SVz0Uc0LVg+R+FbPJS5vGRu/LimSKrOxvt/UPjIuiLd8OlCny4c88Wx36ipTZ2CpA0AhpC0\n26CTV1TUu3a9lStcMBJq+7sN++PqdZ29muulrrtui788xPnTvlLXx39cuVmSdF+yMjvGaFclaQOA\nISRtAGbd2bsxe/L08qErD/G5CStI2gBgCEm7S7h5vePHjwfLy+4NUjbnff/99wfLL7jggnqbCPTN\nXZ/RuX65m8uu5In60DvZ9tFsxxf+6FuF80T67TT7IoXi/Ld1JG0AMISkjaB67zFifb4QncJf+bEx\n/qvC7iG+aNjbmbkjeSR/dqyqqiUkbQAwhKQNwI58LntIado3/+0S9mi5CCRpA4AhJO0us3nzZm/b\nzRkeOxae55s4caK37easL7/88mA50Igon4ienL4pSbov/rpf4cQ5weOunvQTvyBNpfSDkqQ3oqne\nrinMaQMA2oWkjUGxSgTt5BYtFRP22t4/lyT17A/fZ7u42uSdCz44oI7N+2aXIWkDgCEkbQAd66Hk\nnrqPvTF5vIkt6RwkbQAwhKQ9CqVpqj179lSVSdL69etrOtdTTz0VLL/yyivraxwgyc0vz9a/55vZ\n9pL4Ga/W3cnfDdyt++b6q5ncdPXtZ7N7lWyurOzb9dHo5WY2uGOQtAHAEJJ2l+MeIxhJZes6/vu9\nMd7+Unn/HZiwJenuZL0inWq4fZ2IpA0AhpC0AYyYtGQuu94rubuT9fnxjbWrk5G0AcAQkvYo9eyz\nz3rbLtG8++67wfrXXnutt+2Szrx581rQOnS7NJ+t/mctyUuypL04eUyStE/fDB73jeS17Pi8P99T\n+Xtv/6RodM5jD0TSBgBDSNqQxCoRtFmelJ+Ml/rlff3N73dpsLTffcnKQfePJiRtADCEpA1gxN2c\nfHdY9dxc9rLYn8vupitCkjYAGELSNsyljjNnzlSV79y5M3jM2LFjvW2XUB544IFg/TFj6CKon5uL\nfk9jg+XO2Og9Sf3fXPOiPh483//qXG/bzWV3E5I2ABhCjOoSLpXXukqkm+YK0Ur+N8tsqKyWJN2a\nbB+wt7q+4457KPbvUtmN/ZOkDQCGkLQBtN8QV35D6ca5bIekDQCGkLRHgQ0bNnjbaZpq//79wbrX\nX3+9t+2Szvz581vTOHQ1txrkivSXkqTPV56QJH07uUWSVJF/jxy3rORzx76Tb2cFycUT/PN24Vy2\nQ9IGAENI2l2GVSJoJ/fFSH8SP+GVu+5W7HX967ejwnamm+eyHZI2ABhC0gbQNm4ueyhuXXZ68fle\nOVeEJG0AMIWkbYBLHc8880ywfM2aNVXHTJgQfre9p6enFU1E18v64kd1IN/Mtj8T+/fASaJKvjvr\nj59b9D1vfxRlx/3FzlWSJPf5x5WJ67e9zWy0SSRtADCEpD1KsUoEI6IkYW9Pbs2fFftf4R4jSfa4\nvrLWr0W37UPSBgBDSNoAGubWYxcT9veT7BO4p+qMyv1z2XBI2gBgCEm7g7jVICdPngyWL1u2LHjc\n2bNnq8puuOEGb9vNZc+bN6/hdqKbZX2xoiRY7uxOPiZJinRCknTz1vA3Ke370dz88Oz4LWNWSBrw\nyceI1SJFJG0AMISkbZhL4CGsHkEruV7k+uCDsX+nyb5+5u6bXTg+LTzbUvlK+HhUIWkDgCEkbQA1\nK0vYbi67Xn+WfC1/dqKh84xmJG0AMISkPQJcSkmSJFhe/HYZ59ChQ8HyGTNmeNtRFGn16tWNNhMY\ntlW92aqPTx4vfItS/vgHd+7yD8j7+s/i67xid28SlGPQNmCwNxyl8Js2vJGDVnBdcXUlG5xXJdlg\n7Xpbaa/L+6Pry3sr/mB9R/JIM5s5qjE9AgCGkLQBDMkl5Ifi9V55cWlfrVzC7r8yrO883YSkDQCG\nkLRH0MGDB71tl2b27dtX03kefvjhqrJLL720/oYBeeKdrte8badvCvrrf+OV/3LZh4Nn+1q6XJJ0\ndXzAK58SHWusmV2IpA0AhpC0RwFWj6Bl8qu/L8TFrwUrVCvZ4a4eiwl7TzJHkvSvzWllVyFpA4Ah\nJG0Aw7a197bsyaPfG7SeS9izY/8DYT9NPpI94UKwbiRtADCEpN1CLm0cOXIkWL5o0aKazrdhw4Zg\n+Y033lhH6wDJzUZP0qlCcXgue3d0TVa+6R+88ijK6q+8Z9WAs0pS9gW97ksNniVhN4ykDQCGkLQ7\nSD33GBmsHBiu4pcafCXe4u2/NdlecoQ7zn9cH6/1a9NHm4akDQCGkLQBlHIJO+q7S19tx69MevJn\nfEFvs5C0AcAQknYbbN261dt284avvvpqTedZsGBB09oEDBSnZyVJX463eeX/FPsrk9wqkeTfwnPa\nlUpW7u7edzKvNkXcY6RZSNoAYAhJ2xBWj6BdNvbeKUlaXgkn6tIul1fg/titQ9IGAENI2kAXc8l5\neSWby34kuWMEW4PhIGkDgCEk7SZwq0FeeOGFYPmmTZva3iagFi5xz4jyb1PKtye87c9JR/mOT4zb\nHT6BsnuTnNb7vd2sHmkekjYAGELSHgFD3WMEaDXXB9dVsm9X/2GyUNKAe5Dkj6WrRIrfUFNdoXQP\nGkPSBgBDSNpAF+tfd81afytI2gBgCEm7icpWj5w+fbqm88yYMcPbdinovPPOC5YDQ3Ezy69pmrft\n/Iv+0C/Iu9bBsR8Onm/y62ey87h7jkzPDliRrMprHG2gtRgMSRsADCFpdyDuMYJWccn4u5UvSpKW\nJpvzPe8Va0oKrB5x9xYpOXF1H2X1SLORtAHAEJI20I3SkiRd2+GKpzWpPRg2kjYAGELSHkFz5szx\ntt184K5du4L1J0+e3PI2YXQ7qune9pH0EknS7h2Fb6jJ56IfXLzKK3cz1PG0d7xy912QFfnlaD6S\nNgAYQtIeQbWuEmH1CBrl+tDC3qclSbsrN2U7nki8ei5Ru7nrqfH/BM/nErbrmvTQ1iNpA4AhJG2g\nG7lo7KL0Z8IZeWph+7+SiZKkR/Wl/DTcza/dSNoAYEjUhns7d81/wa3+XTKn3Rj394ljP6v0pPeO\nRHNGRFUPrbHP9gX0ktlremjTPL9G6z4V2kHSBgBDmNNuIpIwOl1VD62zz9LTRw5JGwAMYdAGAEPa\nMT3yUhteAwBGk/8o29GO1SMAgCZhegQADGHQBgBDGLQBwBAGbQAwhEEbAAxh0AYAQxi0AcAQBm0A\nMIRBGwAMYdAGAEMYtAHAEAZtADCEQRsADGHQBgBDGLQBwBAGbQAwhEEbAAxh0AYAQxi0AcAQBm0A\nMOT/AR7I9mw9hhc9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c8c28f2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_x = mnist.test.images[:1]\n",
    "digit = render.digit_to_rgb(test_x, scaling = 3)\n",
    "hm = render.hm_to_rgb(relevance, X = test_x, scaling = 3, sigma = 2)\n",
    "digit_hm = render.save_image([digit,hm],'./heatmap.png')\n",
    "data_io.write(relevance,'./heatmap.npy')\n",
    "\n",
    "#display the image as written to file\n",
    "plt.imshow(digit_hm, interpolation = 'none', cmap=plt.cm.binary)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(mnist.test.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(mnist.test.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python3]",
   "language": "python",
   "name": "Python [python3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
