{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# import input_data\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "R = tf.Variable(\"float\", [None, n_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _simple_lrp(R, X, W, b):\n",
    "    Z = tf.expand_dims(W, 0) * tf.expand_dims(X, -1)\n",
    "    Zs = tf.add(tf.expand_dims(tf.reduce_sum(Z, 1), 1),\n",
    "                tf.expand_dims(tf.expand_dims(b, 0), 0))\n",
    "    return tf.reduce_sum(\n",
    "            (Z / Zs) * tf.expand_dims(R, 1),\n",
    "            2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# relevance propagation\n",
    "def relevance_propagation(y_pred, \n",
    "                          reversed_layers_inputs,\n",
    "                          reversed_layers_weights,\n",
    "                          reversed_layers_biases):\n",
    "    assert len(reversed_layers_inputs) == len(reversed_layers_weights) == len(reversed_layers_biases)\n",
    "    R = y_pred\n",
    "    for i in range(len(reversed_layers_inputs)):\n",
    "        R = _simple_lrp(R, \n",
    "                        reversed_layers_inputs[i],\n",
    "                        reversed_layers_weights[i],\n",
    "                        reversed_layers_biases[i])\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron_lrp(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1_activations = tf.nn.tanh(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1_activations, weights['h2']), biases['b2'])\n",
    "    layer_2_activations = tf.nn.tanh(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2_activations, weights['out']) + biases['out']\n",
    "    \n",
    "    R = relevance_propagation(out_layer, \n",
    "                              [layer_2_activations, layer_1_activations, x],\n",
    "                              [weights['out'], weights['h2'], weights['h1']],\n",
    "                              [biases['out'], biases['b2'], biases['b1']])\n",
    "    \n",
    "    return out_layer, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred, R = multilayer_perceptron_lrp(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 7.353227933\n",
      "Epoch: 0002 cost= 2.527427757\n",
      "Epoch: 0003 cost= 1.656883048\n",
      "Epoch: 0004 cost= 1.210896602\n",
      "Epoch: 0005 cost= 0.901563329\n",
      "Epoch: 0006 cost= 0.686827540\n",
      "Epoch: 0007 cost= 0.532962831\n",
      "Epoch: 0008 cost= 0.412098003\n",
      "Epoch: 0009 cost= 0.321827516\n",
      "Epoch: 0010 cost= 0.256299496\n",
      "Epoch: 0011 cost= 0.208806855\n",
      "Epoch: 0012 cost= 0.167910488\n",
      "Epoch: 0013 cost= 0.145477594\n",
      "Epoch: 0014 cost= 0.122694457\n",
      "Epoch: 0015 cost= 0.106957743\n",
      "Optimization Finished!\n",
      "Accuracy: 0.8745999932289124\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy: {}\".format(accuracy.eval({x: mnist.test.images, y: mnist.test.labels})))\n",
    "    \n",
    "    saver.save(sess, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.imshow(mnist.test.images[0].reshape(28, 28), cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "[[  9.53768349 -17.27109528   6.82270432   9.1893034    0.79943144\n",
      "    1.13741088 -34.23589325  39.00849533  -9.30118084  12.30695152]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess, model_path)\n",
    "    predictions = sess.run(pred, feed_dict={x: mnist.test.images[:1]})\n",
    "    print (predictions.shape)\n",
    "    print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess, model_path)\n",
    "    relevance = sess.run(R, feed_dict={x: mnist.test.images[:1]})\n",
    "    print (relevance.shape)\n",
    "    print (relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import render\n",
    "import data_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving image to ./heatmap.png\n",
      "writing data in npy-format to /home/scitator/Documents/Git/lrp_toolbox/tensorflow/heatmap.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/python3/lib/python3.5/site-packages/numpy/core/fromnumeric.py:225: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return reshape(newshape, order=order)\n",
      "/opt/anaconda/envs/python3/lib/python3.5/site-packages/skimage/util/dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAADFCAYAAACfOaMVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncXESkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f702ab5d9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_x = mnist.test.images[:1]\n",
    "digit = render.digit_to_rgb(test_x, scaling = 3)\n",
    "hm = render.hm_to_rgb(relevance, X = test_x, scaling = 3, sigma = 2)\n",
    "digit_hm = render.save_image([digit,hm],'./heatmap.png')\n",
    "data_io.write(relevance,'./heatmap.npy')\n",
    "\n",
    "#display the image as written to file\n",
    "plt.imshow(digit_hm, interpolation = 'none', cmap='brg')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python3]",
   "language": "python",
   "name": "Python [python3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
